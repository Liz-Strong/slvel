{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "Liz Strong, 4/1/2019, modified 4/24/2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('retina')\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data set using pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concatenate all of the various datasets to make the test and the training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_dfs(df_old, df_new):\n",
    "    old_length = len(df_old)\n",
    "    df_old.index = range(old_length)\n",
    "    \n",
    "    new_length = len(df_new)\n",
    "    df_new.index = range(old_length, old_length+new_length)\n",
    "    df_old = df_old.append(df_new)\n",
    "    return df_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.read_pickle(r\"../oam_sim/random_testset1.pkl\")\n",
    "tdf2 = pd.read_pickle(r\"../oam_sim/random_testset2.pkl\")\n",
    "test_dataset = concatenate_dfs(tdf, tdf2)\n",
    "\n",
    "# Concatenate all of the imported dfs\n",
    "test_dataset = concatenate_dfs(tdf, tdf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_pickle(r\"../oam_sim/train_set1.pkl\")\n",
    "df1 = pd.read_pickle(r\"../oam_sim/train_set2.pkl\")\n",
    "df = concatenate_dfs(df, test_df)\n",
    "train_dataset = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure all angles theta are between $\\frac{\\pi}{2l}\\le\\theta\\le\\frac{3\\pi}{2l}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetric_theta_maker(theta, l=4, lower_wedge_angle=np.pi/(2*4), upper_wedge_angle=3*np.pi/(2*4)):\n",
    "    \"\"\"\n",
    "    Make all the simulations have arcs which are located in the same wedge defined by symmetry. \n",
    "    do this by changing the angle (but keeping the radius) of the (R,theta) pair defining the center of the circle\n",
    "    so that it's appropriately placed within the wedge and not outside of it\n",
    "    \n",
    "    Args:\n",
    "        theta (float): angle between center of light and center of orbit in simulations\n",
    "        l (float): OAM azimuthal mode number\n",
    "        lower_wedge_angle (float): lower limit of transformed angles as defined by symmetry\n",
    "        upper_wedge_angle (float): upper limit of transformed angles as defined by symmetry\n",
    "        \n",
    "    Returns:\n",
    "        theta (float): angle beween center of light and center of orbit in simulations, adjusted to take a value between lower_wdge_angle and upper_wedge_angle\n",
    "    \"\"\"\n",
    "    # if angle is negative, make it positive, theta=-theta\n",
    "    if theta<0:\n",
    "        theta *= -1\n",
    "\n",
    "    # if angle is smaller than lower threshold, add the wedge angular spacing to bring it into wedge\n",
    "    if theta <= lower_wedge_angle:\n",
    "        theta += np.pi/l\n",
    "\n",
    "    # if angle is larger than upper threshold, subtract wedge angular spacing till it's within wedge\n",
    "    while theta > upper_wedge_angle:\n",
    "        theta -= np.pi/l\n",
    "\n",
    "    # cause an error if angle is smaller than lower \n",
    "    if theta < lower_wedge_angle:\n",
    "      raise Exception(\"angle is smaller than lower permissible wedge angle\")\n",
    "    if theta > upper_wedge_angle:\n",
    "      raise Exception(\"angle is larger than upper permissible wedge angle\")\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['theta'] = train_dataset['theta'].apply(symmetric_theta_maker)\n",
    "test_dataset['theta'] = test_dataset['theta'].apply(symmetric_theta_maker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the preferences to display all of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use only the fits that have less than 20% max error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dropvals = train_dataset[train_dataset['max_error_normalized']>.2].index\n",
    "train_dataset = train_dataset.drop(dropvals)\n",
    "\n",
    "dropvals = test_dataset[test_dataset['max_error_normalized']>.2].index\n",
    "test_dataset = test_dataset.drop(dropvals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use only counterclockwise==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropvals = test_dataset[test_dataset['clockwise']==1].index\n",
    "test_dataset = test_dataset.drop(dropvals)\n",
    "dropvals = train_dataset[train_dataset['clockwise']==1].index\n",
    "train_dataset = train_dataset.drop(dropvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_dataset), len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make a reference dataset that we can use later to analyze the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reference_dataset = test_dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete some data that doesn't go into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete time step \n",
    "train_dataset.pop(32);\n",
    "test_dataset.pop(32);\n",
    "reference_dataset.pop(32)\n",
    "\n",
    "# delete max_error & RMSE since we're trying to eliminate absolute amplitude measurements\n",
    "train_dataset.pop('max_error');\n",
    "train_dataset.pop('RMSE');\n",
    "test_dataset.pop('max_error');\n",
    "test_dataset.pop('RMSE');\n",
    "reference_dataset.pop('max_error');\n",
    "reference_dataset.pop('RMSE');\n",
    "\n",
    "# delete column against which distance is measured \n",
    "train_dataset.pop(15);\n",
    "test_dataset.pop(15);\n",
    "reference_dataset.pop(15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up indices.\n",
    "amp_start = 5 # This starting value is the position of the column called '7'. EG if there is only ang_vel in front of it, then this number should be 1.\n",
    "amp_end = amp_start + 8\n",
    "mu_start = amp_end\n",
    "mu_end = mu_start + 7\n",
    "sigma_start = mu_end\n",
    "sigma_end = sigma_start + 8\n",
    "vert_offset_position = sigma_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data, amp_start, amp_end, mu_start, mu_end, sigma_start, sigma_end, vert_offset_position, normalized, delta_t=1e-5, l=4):\n",
    "    \"\"\"\n",
    "    Normalize the data which feeds into the model. \n",
    "    Amplitudes normalized by distance between vertical offset and largest amplitude, then scaled by 100.\n",
    "    Mus normalized by sample rate and a factor of 1/20\n",
    "    Sigmas normalized by sample rate and a factor of 1/10\n",
    "    Thetas normalized by pi/(2*l)*30\n",
    "    D scaled by a factor of 1/10\n",
    "    R scaled by a factor of 1/10\n",
    "    ang_vel scaled by a factof of 1/10\n",
    "    offsetx scaled by a factof of 1/10\n",
    "    offsety scaled by a factor of of 1/10\n",
    "    r2 scaled by a factor of 10\n",
    "    \n",
    "    Args:\n",
    "        data (dataframe): data as packaged from multi-Gaussian fitting module\n",
    "        amp_start (int): index in dataframe where the list of the amplitudes begins\n",
    "        amp_end (int): index in dataframe where the list of the amplitudes ends\n",
    "        mu_start (int): index in dataframe where the list of the mu begins\n",
    "        mu_end (int): index in dataframe where the list of the mu ends\n",
    "        sigma_start  (int): index in dataframe where the list of the sigma begins\n",
    "        sigma_end (int): index in dataframe where the list of the sigma ends\n",
    "        vert_offset_position (int): index in dataframe corresponding to the vertical offset\n",
    "        normalized (bool): indicates if the data has previously been normalized or not. If true, normalization not repeated.\n",
    "        delta_t (float): time between samples. sampling rate=1/delta_t\n",
    "        l (float): OAM azimuthal mode index\n",
    "    \n",
    "    Returns:\n",
    "        normalized (bool): indicates that the data has already been normalized. returns True\n",
    "        data (dataframe): dataframe with normalized data\n",
    "    \n",
    "    \"\"\"\n",
    "    data_copy = data.copy()\n",
    "\n",
    "    if normalized == True:\n",
    "        print(\"previously normalized, nothing done.\")\n",
    "        return normalized\n",
    "    else:\n",
    "        print(\"normalizing...\")\n",
    "        ### 1) Treat the amplitudes first\n",
    "        amplitudes = data.iloc[:, amp_start:amp_end] \n",
    "        vert_offset = data.iloc[:, vert_offset_position]\n",
    "        \n",
    "        # find the actual height of the peaks, subtracting the vertical offset (baseline) from the max_amps     \n",
    "        amp_subtr = [amplitudes[a]-vert_offset for a in amplitudes.columns]\n",
    "        for i in range(8):\n",
    "            data.iloc[:, amp_start+i] = amp_subtr[i]\n",
    "            \n",
    "        # change the vertical offset to 0 since above we've adjusted the amplitudes \n",
    "        # such that they're referenced to this vertical offset instead of 0.\n",
    "        vert_offset -= vert_offset\n",
    "\n",
    "        # normalize the amplitudes\n",
    "        max_amplitudes = amplitudes.max(axis=1)\n",
    "        normalized_amplitudes = amplitudes.div(max_amplitudes, axis=0)*100\n",
    "        \n",
    "        data.iloc[:,amp_start:amp_end] = normalized_amplitudes\n",
    "        \n",
    "        ### 2) Normalize the mus with the sample rate\n",
    "        data.iloc[:,mu_start:mu_end] = data.iloc[:,mu_start:mu_end]/delta_t / 20\n",
    "        \n",
    "        ### 3) Normalize the sigmas with the sample rate\n",
    "        data.iloc[:,sigma_start:sigma_end] = data.iloc[:,sigma_start:sigma_end]/delta_t / 10\n",
    "        \n",
    "        ### 4) Normalize theta\n",
    "        data[\"theta\"] = data[\"theta\"] / (np.pi/(2*l))*30\n",
    "        \n",
    "        ### 5) Normalize D\n",
    "        data[\"D\"] = data[\"D\"] / 10\n",
    "        \n",
    "        ### 6) Normalize R\n",
    "        data[\"orbit_R\"] = data[\"orbit_R\"] / 10\n",
    "        \n",
    "        ### 7) Make angular velocity smaller\n",
    "        data[\"ang_vel\"] = data[\"ang_vel\"]/10\n",
    "        \n",
    "        ### 8) Make offsets smaller\n",
    "        data[\"offsetx\"] = data[\"offsetx\"]/10\n",
    "        data[\"offsety\"] = data[\"offsety\"]/10\n",
    "\n",
    "        ### 9) Make r2 bigger\n",
    "        data[\"r2\"] = data[\"r2\"]*10\n",
    "        \n",
    "        \n",
    "        ### 10) Change status of data set normalization from False to True so we don't normalize more than once.\n",
    "        normalized = True\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # if the amplitudes were 0 to start, make them 0 again.\n",
    "        data = data.where(data_copy != 0, 0.0)\n",
    "        return normalized, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this to false to normalize. once the data has been normalized, this will take the value True so so it can't be normalized again.\n",
    "normalized_dataset = False\n",
    "normalized_testset = False\n",
    "normalized_referenceset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_dataset, train_dataset = normalize_data(train_dataset, amp_start, amp_end, mu_start, mu_end, sigma_start, sigma_end, vert_offset_position, normalized_dataset)\n",
    "normalized_testset, test_dataset = normalize_data(test_dataset, amp_start, amp_end, mu_start, mu_end, sigma_start, sigma_end, vert_offset_position, normalized_testset)\n",
    "normalized_refset, reference_dataset = normalize_data(reference_dataset, amp_start, amp_end, mu_start, mu_end, sigma_start, sigma_end, vert_offset_position, normalized_referenceset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = train_dataset.describe()\n",
    "train_stats = train_stats.transpose()\n",
    "train_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[4,4])\n",
    "for i in train_dataset.orbit_R.unique():\n",
    "    ds = train_dataset.loc[train_dataset['orbit_R']==i]\n",
    "    unique_ang_vels = ds.ang_vel.unique()\n",
    "    plt.plot(i*np.ones(len(unique_ang_vels)), unique_ang_vels, 'k.')\n",
    "plt.ylabel('angular velocity, $\\Omega$ [1/s]')\n",
    "plt.xlabel('Orbit Radius, R [pixels]')\n",
    "plt.title('R-$\\Omega$ parameter space')\n",
    "\n",
    "\"\"\"\n",
    "for i in [1050, 1100, 1150]:\n",
    "    unique_ang_vels = dataset.ang_vel.unique()\n",
    "    plt.plot(i*np.ones(len(unique_vorticities)), unique_ang_vels, 'rx')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get rid of identifying data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get rid of column with orbit radius--for now--we'll use this to sort later.\n",
    "#dataset.pop('orbit_R')\n",
    "train_dataset.pop('offsetx')\n",
    "train_dataset.pop('offsety')\n",
    "train_dataset.pop('clockwise')\n",
    "train_dataset.pop('counterclockwise')\n",
    "train_dataset.pop('avg_SNR')\n",
    "#train_dataset.pop(\"D\")\n",
    "#train_dataset.pop('theta')\n",
    "train_dataset.pop(31) # vert offset\n",
    "train_dataset.tail()\n",
    "\n",
    "\n",
    "# get rid of column with orbit radius--for now--we'll use this to sort later.\n",
    "#dataset.pop('orbit_R')\n",
    "test_dataset.pop('offsetx')\n",
    "test_dataset.pop('offsety')\n",
    "test_dataset.pop('clockwise')\n",
    "test_dataset.pop('counterclockwise')\n",
    "test_dataset.pop('avg_SNR')\n",
    "#test_dataset.pop(\"D\")\n",
    "#test_dataset.pop('theta')\n",
    "test_dataset.pop(31) # vert offset\n",
    "test_dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete data with R<60\n",
    "dropvals = test_dataset[test_dataset['orbit_R']>60].index\n",
    "test_dataset = test_dataset.drop(dropvals)\n",
    "dropvals = train_dataset[train_dataset['orbit_R']>60].index\n",
    "train_dataset = train_dataset.drop(dropvals)\n",
    "dropvals = ref_dataset[ref_dataset['orbit_R']>60].index\n",
    "ref_dataset = ref_dataset.drop(dropvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count the number of unique angular velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"number unique angular velocities in training dataset\", train_dataset['ang_vel'].nunique())\n",
    "\n",
    "print(\"number unique angular velocities in test dataset\", test_dataset['ang_vel'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean the data of any unknown values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.isna().sum();\n",
    "train_dataset.isna().sum();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the rows with na\n",
    "train_dataset = train_dataset.dropna()\n",
    "test_dataset = test_dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_stats = train_dataset.describe()\n",
    "#train_stats.pop(1)\n",
    "#train_stats.pop('ang_vel')\n",
    "#train_stats.pop('orbit_R')\n",
    "\n",
    "train_stats = train_stats.transpose()\n",
    "train_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation=tf.nn.relu, input_shape=[len(train_dataset.keys())]),\n",
    "        layers.Dense(64, activation=tf.nn.relu),\n",
    "        layers.Dense(len(train_labels.keys()))\n",
    "    ])\n",
    "    optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                 optimizer=optimizer,\n",
    "                 metrics=['mean_absolute_error','mean_squared_error','mean_absolute_percentage_error','acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training progress by printing a single dot for each completed epoch\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch % 100 == 0: print('')\n",
    "        print('.',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error [1/s]')\n",
    "    plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n",
    "           label = 'Val Error')\n",
    "    plt.plot(hist['epoch'], hist['mean_absolute_error'],\n",
    "           label='Train Error')\n",
    "\n",
    "    #plt.ylim([0,5])\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Square Error [$1/s^2$]')\n",
    "\n",
    "    plt.plot(hist['epoch'], hist['val_mean_squared_error'],\n",
    "           label = 'Val Error')\n",
    "    plt.plot(hist['epoch'], hist['mean_squared_error'],\n",
    "           label='Train Error')\n",
    "    #plt.ylim([0,20])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(hist['epoch'],hist['val_mean_absolute_percentage_error'],label='val mape')\n",
    "    plt.plot(hist['epoch'],hist['mean_absolute_percentage_error'],label='training mape')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('mean absolute percentage error')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels_to_sep = [\"ang_vel\", \"orbit_R\"]\n",
    "\n",
    "all_labels_to_eliminate = [\"ang_vel\", \"orbit_R\", \"D\", \"theta\"]\n",
    "\n",
    "\n",
    "\n",
    "# separate the features from the labels\n",
    "train_labels = train_dataset[labels_to_sep]\n",
    "test_labels = test_dataset[labels_to_sep]\n",
    "\n",
    "\n",
    "for l in all_labels_to_eliminate:\n",
    "    train_dataset.pop(l)\n",
    "    test_dataset.pop(l)\n",
    "\n",
    "# dont apply more normalization for now\n",
    "normed_train_data = train_dataset\n",
    "normed_test_data = test_dataset\n",
    "\n",
    "# build the model\n",
    "model = build_model()\n",
    "\n",
    "# inspect the model\n",
    "example_batch = normed_train_data[:10]\n",
    "example_result = model.predict(example_batch)\n",
    "\n",
    "# train the model\n",
    "\n",
    "# patience parameter is the number of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
    "EPOCHS = 400\n",
    "history = model.fit(normed_train_data, train_labels, epochs=EPOCHS,\n",
    "                    validation_split = 0.2, verbose=0, callbacks=[early_stop, PrintDot()])\n",
    "\n",
    "plot_history(history)\n",
    "model.save('model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we'll see how the model generalizes using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mae: mean averaged error, mse: mean squared error\n",
    "loss, mae, mse, mpe, acc = model.evaluate(normed_test_data, test_labels, verbose=0)\n",
    "print(\"Testing set Mean Abs Error: {:5.2f} 1/s\".format(mae))\n",
    "print(\"\\nTesting set mean square error: {:5.2f} 1/s^2\".format(mse))\n",
    "print(\"\\nTesting set mean absolute percentage error: {:5.2f}\".format(mpe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use model to make predictions\n",
    "test_predictions = model.predict(normed_test_data)\n",
    "predicted_ang_vels = [test_predictions[i][0] for i in range(len(test_predictions))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_dataset = reference_dataset.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qoi = ref_dataset.iloc[:]['ang_vel'].to_numpy()\n",
    "x_values, y_values = qoi, predicted_ang_vels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_means_plot_maker(x_values, y_values, num_bins=30, w=15, x_label='x', y_label=' y'):\n",
    "    \"\"\"\n",
    "    Generate figure to summarize results statistically. Bin results into num_bins and within each, plot a mean and standard deviation.\n",
    "    \n",
    "    Args:\n",
    "        x_values (array):\n",
    "        y_values (array):\n",
    "        num_bins (int): the number of segments within which we average.\n",
    "        w (float): the width of the violin plots\n",
    "        x_label (string):\n",
    "        y_label (string): labels for the horizontal and vertical axes of the figure, respectively\n",
    "        \n",
    "    Returns:\n",
    "        -\n",
    "    \"\"\"\n",
    "\n",
    "    min_x, max_x = np.min(x_values), np.max(x_values)+.1 # add this .1 on so that points with this last value are included in the averaging below.\n",
    "    bin_locations = np.linspace(min_x, max_x, num_bins+1, endpoint=True)\n",
    "\n",
    "    #find all y values whose positions correspond to the x values of the bin locations\n",
    "    args_initial = [np.argwhere((x_values>=bl1) & (x_values<bl2)).flatten() for bl1, bl2 in zip(bin_locations[:-1], bin_locations[1:])]\n",
    "    bin_centers_initial = [np.mean([bl1, bl2]) for bl1, bl2 in zip(bin_locations[:-1], bin_locations[1:])]\n",
    "\n",
    "    # make sure that if any of these bins made above don't have any elements in them, they get deleted.\n",
    "    args = [a for a in args_initial if a.size!=0] #get rid of any empty ones\n",
    "    bin_centers = [b for a,b in zip(args_initial, bin_centers_initial) if a.size!=0]\n",
    "\n",
    "    predictions = [np.asarray(y_values)[a] for a in args]\n",
    "    positions = np.array(bin_centers)\n",
    "    std_devs = [np.std(p) for p in predictions]\n",
    "    means = [np.mean(p) for p in predictions]\n",
    "    \n",
    "\n",
    "    # Create plot\n",
    "    plt.figure(899, figsize=(10,7))\n",
    "    plt.plot([0, 1200], [0,  1200], 'k--')\n",
    "    plt.plot(positions, means, 'k.')\n",
    "    plt.errorbar(positions, means, yerr=std_devs, ls='none', capsize=3)\n",
    "    \n",
    "    plt.xlabel(x_label, fontsize=14)\n",
    "    plt.ylabel(y_label, fontsize=14)\n",
    "    \n",
    "\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.xlim(0, 1200)   \n",
    "\n",
    "   \n",
    "    # Create plot\n",
    "    plt.figure(900, figsize=(10,5))\n",
    "    plt.plot([0, max_x*1.1], [0, 0], 'k--')\n",
    "    plt.plot(positions, means-positions, '.')\n",
    "     \n",
    "    plt.xlabel(x_label, fontsize=14)\n",
    "    plt.ylabel(y_label+' means', fontsize=14)\n",
    "    plt.xlim(0, 1200)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.ylim([-200,200])\n",
    "    \n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(901, figsize=(10,5))\n",
    "    plt.plot(positions, np.asarray(std_devs)/np.asarray(positions)*100, '.')\n",
    "    plt.xlabel(x_label, fontsize=14)\n",
    "    plt.ylabel('standard deviations as a percentage of position', fontsize=14)\n",
    "    plt.ylim(0, 100)#np.max(std_devs)*1.5)\n",
    "    plt.xlim(0, 1200)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "\n",
    "    \n",
    "    print(\"error%:\", np.asarray(std_devs)/np.asarray(positions)*100, \"average error normalized:\", np.mean(np.asarray(std_devs)/np.asarray(positions)*100))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "analyze_means_plot_maker(np.asarray(x_values)*10, 10*np.asarray(y_values), num_bins=25, x_label='actual angular velocity', y_label='predicted-actual angular velocity')\n",
    "plt.figure(899)\n",
    "plt.ylim(0,1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
